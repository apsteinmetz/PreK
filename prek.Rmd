---
title: 'Exploring NYC Pre-K seats vs. Neighborhood Income'
author: 'Art Steinmetzz'
date: '13 January 2017'
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 7
    fig_height: 4.5
    theme: readable
    highlight: tango
---

# Introduction

A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families.  When the program was initially rolled out there were complaints in some quarters that upper-income neighborhoods were getting more slots.

This is an exploration comparing income to pre-K seats by neighborhoods. It was done mainly to help me practice with web scraping, document parsing and making cool bi-variate choropleth maps!  I had to invent a novel method to get good looking bivariate legend onto the chart.

In my original version I use an outside program, PDFTOTEXT.EXE, to get parseable text out of the PDF documents at the NYC.gov web site.  I share the code for this here but skip the step in the notebook since Kaggle can't handle the outside program (as far as this noob knows).  Instead, I provide the raw converted text files to illustrate the parsing.

A further complication is to directly grab the income and population data from the census bureau requires an API key.  You'll have to get your own here: http://api.census.gov/data/key_signup.html.  I comment out the relevant lines but instead provide the raw downloaded data sets to illustrate how they get manipulated.





# Load Libraries

```{r,message=FALSE}
library(acepack)
library(Hmisc)
library(latticeExtra)
library(gridExtra)
library(htmlTable)
library(choroplethr)
#not on CRAN. Do an install the first time
#devtools::install_github('arilamstein/choroplethrZip@v1.5.0')
library(choroplethrZip)
library(acs)
library(stringr)
library(dplyr)
library(reshape2)
library(ggplot2)
#not on CRAN. Do an install the first time
#devtools::install_github("wilkelab/cowplot")
library(cowplot)

```

# Load Data

First, we go to the American Community Survey from the US census. The 'acs' package lets us directly grab that data.  It's not exactly easy because the breadth of the data is huge and it took a lot of trial and error to get just the desired data.  To make it work we need to browse around the ACS to find out the NYC FIPS codes that will map to NYC zip codes and find the table numbers that hold the income and population data.  Start your browsing here: https://factfinder.census.gov/faces/nav/jsf/pages/searchresults.xhtml?refresh=t

## Use the acs package to construct the queries for census api

```{r,message=FALSE}
# -----------------------------------------------------------------------
# get census data on children and income
#census api key
#see acs package documentation
#api.key.install('your key here')

# NYC county codes
nyc_fips = c(36085,36005, 36047, 36061, 36081)
#get the zips for all nyc counties
data("zip.regions")
nyc_zips<-data.frame(county.fips.numeric=nyc_fips)%>%inner_join(zip.regions)%>%select(region)%>%t
# make an ACS geo set
nycgeo<- acs::geo.make(zip.code = nyc_zips)
```
##Connect to census.gov.  Requires API key  You can uncomment the lines below
if you have a key. Otherwise skip to the next section to load the raw csv files
```{r,message=FALSE}

# income<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number="B19301")
# #get relevant data into a data frame format
# inc<-cbind(acs::geography(income),acs::estimate(income))
# kidsUnder3<-acs::acs.fetch(endyear=2011,geography=nycgeo,table.number="B09001",keyword = "Under 3")
# kids<-cbind(acs::geography(kidsUnder3),acs::estimate(kidsUnder3))
# totalPop<-acs.fetch(endyear=2011,geography=nycgeo,table.number="B01003")
# pop<-cbind(geography(totalPop),estimate(totalPop))

```

##Load from csv files the data we would have otherwise gotten from census.gov
```{r}


#if we can't connect to census.gov
inc<-read.csv('NYCincome.csv')
kids<-read.csv('NYCkids.csv')
pop<-read.csv('NYCpopulation.csv')

```

##Massage the census data
```{r}


names(inc)<-c("NAME","zip","perCapitaIncome")
#needs some cleanup of dupes. I don't know why
inc<-distinct(select(inc,zip,perCapitaIncome))

#kids under 3 in 2011 should approximate Pre-K kids in 2015
names(kids)<-c("NAME","zip","kidsUnder3")
kids<-distinct(select(kids,zip,kidsUnder3))
kids<-kids%>%select(zip,kidsUnder3)%>%distinct()%>%filter(kidsUnder3!=0 | kidsUnder3!=NA)

names(pop)<-c("NAME","zip","totPop")
pop<-pop%>%select(zip,totPop)%>%distinct()%>%filter(totPop!=0)

census<-pop%>%inner_join(kids)%>%inner_join(inc)%>%mutate(zip=as.character(zip))
```

## Look at some preliminary pictures
So now we have some census data.  We can use the 'chorplethr' package to
easily create some meaningful maps.  Lets look at where the kids are and what incomes are in NYC Zip Codes. 

```{r,message=FALSE}

#where are zips with the most rugrats?
kidsChor <- census%>%transmute(region=zip,value=kidsUnder3/totPop*100)
zip_choropleth(kidsChor,zip_zoom = nyc_zips,title = "Percentage of Kids Under 3 in 2011")
incomeChor <- census%>%transmute(region=zip,value=perCapitaIncome)
zip_choropleth(incomeChor,zip_zoom = nyc_zips,title = "Per Capita Income 2011")

```
# Get the Pre-K data

As we did before we have two procedures, one that illustrates downloading the
PDF pre-K brochures from NYC.gov and converting them to text, which I comment out.  The second loads the converted text in case the PDF's cease to be available.

We then parse it to find the Zip codes of the schools, the number of seats and whether they are full-day or half-day.

## Download PDFs from NYC.gov

Convert to text using an outside program, PDFTOTEXT.EXE (http://www.foolabs.com/xpdf/home.html).

```{r}
# # -----------------------------------------------------------------------
# # get NYC data on pre-K programs
# # scan seat directory pdfs and put into a data frame by zip code
# #DOE pre-k directories
# urls<- c("http://schools.nyc.gov/NR/rdonlyres/1F829192-ABE8-4BE6-93B5-1A33A6CCC32E/0/2015PreKDirectoryManhattan.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/5337838E-EBE8-479A-8AB5-616C135A4B3C/0/2015PreKDirectoryBronx.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/F2D95BF9-553A-4B92-BEAA-785A2D6C0798/0/2015PreKDirectoryBrooklyn.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/B9B2080A-0121-4C73-AF4A-45CBC3E28CA3/0/2015PreKDirectoryQueens.pdf",
#          "http://schools.nyc.gov/NR/rdonlyres/4DE31FBF-DA0D-4628-B709-F9A7421F7152/0/2015PreKDirectoryStatenIsland.pdf")
# 
# #assumes pdftotext.exe is in the current directory.  Edit as necessary
# exe <- "pdftotext.exe"
# 
# #regex to parse address line
# pkseattokens <-"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+) ([0-9]{1,4}) (FD|HD|AM|PM|5H)"
# 
# # each of the PDF directories have 27 pages of intro material. Skip it. This might change for different years. Check PDFs
# firstPage = 28
# 
# dests <- tempfile(str_match(urls,"Directory(\\w.+).pdf")[,2],fileext = ".pdf")
# txt<- NULL
# for (i in 1:length(urls)) {
#   download.file(urls[i],destfile = dests[i],mode = "wb")
#   # pdftotxt.exe is in current directory and convert pdf to text using "table" style at firstpage
#   result<-system(paste(exe, "-table -f", firstPage, dests[i], sep = " "), intern=T)
#   # get txt-file name and open it  
#   filetxt <- sub(".pdf", ".txt", dests[i])
#   txt <- append(txt,readLines(filetxt,warn=FALSE))
# }

```
## Alternatively, import and combine the already converted text files.

```{r}
boroughList <- c('Manhattan','Bronx','Brooklyn','Queens','Staten')
txt<-NULL
for (borough in  boroughList){
  # get txt-file name and open it  
  filetxt <- paste(borough, ".txt", sep='')
  txt <- append(txt,readLines(filetxt,warn = FALSE))
}

```
## Extract relevant info from text files

Pull out the Zip, seat count and day length of each school.  Note the pretty heroic (for me, anyway) regex, pkseattokens.

```{r}
# find address line which contains zip and seat count
txt2<-txt[grep("Address:",txt)]
# strip chars that will mess up regex
pkseattokens <-"(Address: )([.[:alnum:]- ()]+),+ ([0-9]{5})([a-zA-Z .()-:]+)([0-9]{1,4}) (FD|HD|AM|PM|5H)"
txt2<-sub("'","",txt2)
seats<-as.data.frame(str_match(txt2,pkseattokens))[,c(4,6,7)]
names(seats)<-c("zip","seats","daylength")
seats$seats<-as.integer(seats$seats)
# aggregate seat count by zip code
sumSeats<-seats%>%group_by(zip)%>%summarise(count=n(),numSeats=sum(seats,na.rm=TRUE))
names(sumSeats)<-c("zip","schools","numSeats")

```
## Look at Day lengths
We won't use this data further in our analysis, but lets look at how
many seats are full day vs. something else.  Full day is the overwhelming majority.
```{r}
#how do the programs break out in terms of day length?
sumDayLength<-seats%>%group_by(daylength)%>%summarise(NumSchools=n(),NumSeats=sum(seats,na.rm=TRUE))
print(sumDayLength)

```

