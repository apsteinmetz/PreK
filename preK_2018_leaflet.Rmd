---
title: "Pre-K 2018"
output:  html_document
---

# A Geographic Exploration of the NYC Free Pre-K program

## Preface: What I will cover

Getting data from PDF files, census data, TIGER map data in "simple features" form and mashing it all together.
Choropleth maps using multiple packages for comparison.
Introducing a unique scheme to create bi-variate legends for maps.

## Introduction

This is an update of my earlier exploration of the NYC Pre-Kindergarden program through bi-variate choropleth maps.  As I looked at what I originally did two years ago it looked so crude, mainly becuase the R toolchain has made amazing strides recently.  There has been an explosion of package functionality!  Then, I used static maps, shapefiles and an external PDF-to-text program and my methods were not entirely "tidy," partly because few R packages used the tidy paradigm.  Now we have `tidycensus` and `sf` which uses "simple features" data frames instead of shapefiles.  Personally, I find simple features objects so, well, simple and they are extensions of the familiar R data frame.  The addition of `geom_sf()` to ggplot2 opens up enormous capability.   Further, packages like `leaflet` and `plotly` now allow very sophisticated interactive mapping.  The `mapview` package makes it crazy simple to access interactive mapping.  All this evolution compelled me to go back to this project, sourcing updated data, of course.

A hallmark of mayoral administration of NYC Mayor Bill DeBlasio has been free pre-K for all New York families.  When the program was initially rolled out there were complaints in some quarters that upper-income neighborhoods were getting more slots ( http://www.theatlantic.com/education/archive/2015/02/a-tale-of-two-pre-ks/385997/).  We will examine this propostion visually, using a bi-variate choropleth map.

One cool thing about this exploration is we will weave together together three distinct rivers of data.  

First, is the information about the pre-K program from the city.  We are mainly interested in the number of seats by zip code. To get this we have to download a separate directory for each borough in PDF format and scrape each one.  `pdftools` gets the job done.  This package is now in version 2.00 and adds some ability to extract tables, not just text, but the format of the city's directories is too "pretty" so we have design our own table parsing routines.

Second, we need information about the number of pre-K-age children and income by each zip code. We'll get this from the census American Community Survey (which is conducted more frequently than the decennial census).  Thanks to `tidycensus` this is quite easy!

Finally, we need geographic information about zip code boundaries.  The TIGER data at the census is now available in `sf` format thanks to the `tigris` package.

The cup of the geospatial analyst using R really runneth over!  To make the point I'll show five different paths to making this map, base R, ggplot2 and mapview (the easy ways), leaflet and plotly (the fancy ways).

Despite this embarassement of riches the one thing we still have to hand roll is a chart legend suitable for a bivarate map.  I have a good-looking solution but it is still a hack. If I ever get the courage to make a pull request in ggplot2, I might propose a `legend_bv` thing.

Thanks to Joshua Stevens for the inspiration and color theory of bi-variate maps (http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/).

Note that getting data from the census bureau requires a free API key.  You'll have to get your own here: http://api.census.gov/data/key_signup.html.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidycensus)
library(tigris)
library(sf)
library(pdftools)
library(mapview)
library(leaflet)
library(plotly)
```
# Download NYC PRE-K PDF Directories

Throughout this notebook you'll notice I save data files at intermediate steps and include code that skips sections if the relevant data file already exists.  It makes it easy to change bits of the notebook and quickly re-run the whole thing.
```{r}

# -----------------------------------------------------------------------
# get NYC data on pre-K programs in 2018
# scan seat directory pdfs and put into a data frame by zip code

##DOE pre-k directories. URLs valid as of November 2018
base_url <- "https://www.schools.nyc.gov/docs/default-source/default-document-library/"

urls<-c(paste0(base_url,"2018nycprekdirectorybronx-english"),
        paste0(base_url,"nyc-pre-kindergarten-directory-brooklyn"),
        paste0(base_url,"2018-nyc-prek-directory-manhattan-english"),
        paste0(base_url,"2018nycprekdirectoryqueens-english"),
        paste0(base_url,"2018nycprekdirectorystatenislandenglishweb-english"))

boroughs <- c('Bronx','Brooklyn','Manhattan','Queens','Staten')

dests <- paste0("pdf/",boroughs,"2018_Pre_K.pdf")

# Download PDF directories from NYC if PDFs are not already present
if (!is.na(match(FALSE,file.exists(dests)))) {
  for (i in 1:length(urls)) {
    download.file(urls[i],destfile = dests[i],mode = "wb")
  }
}
```

## Extract Text from PDFs

Select only those pages that are directory listings and split into lines.  The `pdf_text` function splits each page into a list item of text.  To identify those pages which have listings we chose a term that is unlikely to appear elsewhere in the document.  I chose "Playspace."  It only appears as a false positive once in the interpretive key.

![](img/one_listing.png)

```{r}
if (!file.exists("data/schools_2018.rdata")){
  # extract and combine text from PDFs
  listings <- NULL
  for (i in 1:length(dests)) {
    print(dests[i])
    txt <- suppressMessages(pdf_text(dests[i]))
    # "Playspace:" is marker for actual directory listing of pre-k schools on that page
    # Discard other pages
    txt <- txt[str_detect(txt,"Playspace:")]
    txt <- txt[-1] #first instance is an example page. discard.
    listings <- append(listings,txt)
    # file.remove(dests[i])
  }
  
  #text is in one page per item
  #divide listings into separate lines
  listings <- listings %>% str_split("\\r?\\n") %>% unlist()
}
```
Now it's matter of coming up with appropriate regexes to extract the relevant fields and to determine the boundaries between records.

```{r}
if (!file.exists("data/schools_2018.rdata")){
  # regex to extract valid address
  addresstokens <-"(Address: ).+([0-9]{5})"
  #use lines containing address as anchor to mark record boundaries
  lines_address<-grep(addresstokens,listings)
  record_start<-lines_address-1
  record_end<-(lines_address-2)[-1] %>% append(length(listings))
  
  #----------------------------
  # workhorse function to create a record with all relevant info about a school
  create_record <- function(rec_index){
    rec      <- listings[record_start[rec_index]:record_end[rec_index]]
    name     <- str_extract(rec[1],"(.+)(?=\\|)") %>% str_trim()
    district <- str_extract(rec[1],"(?<=\\| )[0-9]{2}") %>% str_trim() %>% as.integer()
    address  <- str_extract(rec[2],addresstokens)%>% str_remove("Address: ")
    zip      <- str_extract(address,"[0-9]{5}$")
    address  <- str_remove(address," [0-9]{5}$")
    borough  <- address %>% str_extract("[A-z]+(?= NY)")
    # the next two searches should be tolerant of not knowing line the field is on
    # but assumes the first satisfaction of the regex is the relevant one.
    day_length <- str_extract(rec,"([A-Za-z]+)(?=-Day)")  %>% 
      na.omit() %>% 
      as.character() %>% .[1]
    seats <- str_extract(rec,"(?<=Seats: )[0-9]+") %>% 
      na.omit() %>% 
      as.integer() %>% .[1]
    return(data_frame(name=name,
                      address=address,
                      zip=zip,
                      borough = borough,
                      district = district,
                      seats = seats,
                      day_length=day_length))
    
  }
  #----------------------------
  
  #create data frame with a line for each school
  schools_2018 <- 1:length(record_start) %>% map(create_record) %>% bind_rows()
  # gussy it up a bit
  schools_2018 <- schools_2018 %>% 
    mutate(borough = ifelse(borough=="Island","Staten",borough)) %>% 
    select(borough,zip,everything()) %>% 
    arrange(borough,zip)
  save(schools_2018,file="data/schools_2018.rdata")
knitr::kable(schools_2018[1:10,])
```
}

load("data/schools_2018.rdata")

Now that's what I'm talkin' about!  One of the most satisfying things in data science, for me, is to take messy data and make it nice and tidy.  You would think this has implications for how I clean my bathroom, but, no.

#Get census data
Income, population, population of school age children and census zip code geometries.

Here, I am making an aesthetic choice. I want to use wide, not tidy data because duplicating the zip code geometries for every variable seems wasteful and doing math on multiple variables is easier with wide data.  `get_acs` will optionally return wide data but I don't like the column names. It's easy enought to `spread` the data ourselves.

`tidycensus` has a great feature in that it can return an `sf` object with the geometries already provided but I won't use it because, for some unknown reason, when fetching by zip codes you must get EVERY zip code in the USA, when you include the geometries that's a lot of data to suck up.  We can be much more efficient by getting the census data and TIGER data separately, then filtering for NYC zip codes.

```{r message=FALSE, warning=FALSE}
if (!file.exists("data/acs_dat.rdata")){
  
  # I keep my census api key in a system variable. This adds it to the r environment 
  # with a label that tidycensus expects.
  #if (nchar(Sys.getenv("CENSUS_API_KEY"))==0) 
  
  #census_api_key(Sys.getenv("CENSUS_API_KEY"))
  
  options(tigris_use_cache = TRUE)
  
  # get zip code boundaries from Tigris
  geo_nyc_zips_raw <-zctas(cb = FALSE, 
                           starts_with = c("10","11"),
                           state = "NY",
                           year = 2010,
                           class = "sf")
  
  geo_nyc_zips <- geo_nyc_zips_raw %>% rename(zip= ZCTA5CE10) %>% select(zip,geometry)
  
  #The returned set of geometries includes Long Island and Westchester so
  #crop the bounding box to get a better sized map.
  nyc_box<-c(xmin=-74.5,ymin=40.5,xmax=-73.6,ymax=41.0)
  geo_nyc_zips <-geo_nyc_zips %>%  st_crop(nyc_box)
  save(geo_nyc_zips,file="data/geo_nyc_zips.rdata")
  # FYI if you want to see all the available acs tables from the census bureau
  # v15 <- load_tvariables(2016, "acs5", cache = TRUE)
  # View(v15)
  
  acs_table_pop <- "B01003_001" # total population
  acs_table_kid <- "B05009_002" # children under 6
  acs_table_inc <- "B19013_001" # median income, last 12 months
  # NYC county codes
  # this comes from USPS web site
  # get the file from my github repo if you like
  nyc_zips <- read_tsv("data/nyc zip codes.txt") %>% rename(zip=ZIP) 
  
  # we can only use the zcta (zip code) geography if we fetch the whole country!
  acs_raw <- get_acs(geography = "zcta",
                     variables = c(HouseholdIncome = acs_table_inc,
                                   kidsUnder3 = acs_table_kid,
                                   totPop = acs_table_pop))
  # filter for just NYC zip codes
  # and make the data wide
  acs_dat<- acs_raw %>% 
    rename(zip=GEOID) %>% 
    right_join(nyc_zips) %>% 
    select(zip,variable,estimate) %>% 
    filter(!is.na(variable)) %>% 
    spread(variable,estimate) %>% 
    filter(!is.na(HouseholdIncome)) %>% 
    {.}
  
  save(acs_dat,file="data/acs_dat.rdata")
}
load("data/acs_dat.rdata")
acs_dat
```
# Get to work
Now we have everything we need to make some maps: the schools data from the city, the population and income data from the census and the map coordinates for all NYC zip codes. Let's bring them together.

```{r}
load("data/schools_2018.rdata")
load("data/alldata2018.rdata")
load("data/geo_nyc_zips.rdata")


# aggregate seat count by zip code

sum_seats <- schools_2018 %>% 
  group_by(zip) %>% 
  summarise(schools = n(), 
            numSeats = sum(seats, na.rm = TRUE))

all_data_2018 <- geo_nyc_zips %>% 
  left_join(acs_dat,by="zip") %>% 
  left_join(sum_seats,by="zip") %>% 
  # remove non-NYC geometries
  filter(!is.na(HouseholdIncome)) %>% 
  # some tiny zips have people but no schools.  Set schools to zero.
  mutate(schools=ifelse(is.na(schools),0,schools)) %>% 
  mutate(numSeats=ifelse(schools==0,0,numSeats)) 
  
#add per-capita variables
all_data_2018 <- all_data_2018 %>% 
  mutate(seatsPer100Kids = round(numSeats/kidsUnder3*100,digits=1),
         seatsPer1000People=round(numSeats/totPop*1000,digits=1)) %>%
  mutate(seatsPer100Kids=ifelse(is.nan(seatsPer100Kids),0,seatsPer100Kids))
#compute income quantiles
fn<-ecdf(all_data_2018$HouseholdIncome)
all_data_2018 <- all_data_2018 %>% 
  mutate(incomeQuantile=fn(HouseholdIncome))
#compute seatsPer100Kids quantiles
fn<-ecdf(all_data_2018$seatsPer100Kids)
all_data_2018 <- all_data_2018 %>% 
  mutate(seatsQuantile=fn(seatsPer100Kids))
# aggregate quantiles into three categorical bins
bins<-3
all_data_2018 <- all_data_2018 %>% 
  mutate(seatsBin=as.factor(cut(seatsQuantile,breaks=bins,ordered_result = T,labels=F,include.lowest = T))) %>% 
  mutate(incomeBin=as.factor(cut(incomeQuantile,breaks=bins,ordered_result = T,labels=F,include.lowest = T)))
#recode numeric bins to descriptions
levels(all_data_2018$seatsBin) = c("Few","Average","Many")
levels(all_data_2018$incomeBin) = c("Low","Average","High")

save(all_data_2018,file="data/alldata2018.rdata")
```
#Are Wealthy Neighborhoods Getting Too Many Seats?

Initially, we can just view a scatter plot of the quantiles.  Here we see that there is scant relationship between zip code income and per-child seats over most of the range.  There is a cluster of high-income zip codes at the lower end of the seat count.  This alone discounts the notion that the wealthy are getting more than their fair share of seats.  This "merely" visual approach tells us enough.  We might leave it at that, content that there is no evidence of social injustice, but this is an issue where maps can tell a much richer story.
```{r message=FALSE}
ggplot(all_data_2018,aes(incomeQuantile,seatsQuantile))+geom_point()+geom_smooth()

```

# Make Some Pretty Maps

We are used to seeing univarate color maps but here we have two values, seats and income.  How might we code them into one variable. Given the social justice element of the original question, we might create a single scale of good to bad. So, for our purposes, high income and low seats is "good" along with low income and high seats.  Simply taking the difference between income and seats quantiles works.  The largest differences are "good."

```{r}
#combine income and seats into single metric
#bigger is better
all_data_2018 <- all_data_2018 %>% 
  mutate(disparity = abs(seatsQuantile - incomeQuantile))
```

I will present maps generated by several different packages because I can't leave well enough alone.  All are great but have varying aesthetics.  You can pick what appeals to your taste.  Nowadays, we often ignore the base plot functions but the `sf` package includes a `plot` method for sf objects.  so you can create a good looking map with no work at all.

```{r}
all_data_2018 %>% select(disparity) %>% plot()
```

Whaat?  That's it?  It looks great!  You can clearly see by the lighter colors where the disparity between income and seats is the widest.  If base R graphics are your thing, you can now annotate to your hearts content.

The `plot` method will automatically facet the plot if you include more than one feature column.  Let's look at the seats and income quantiles.
```{r}
all_data_2018 %>% select(seatsQuantile,incomeQuantile) %>% plot()
```

Now let's make the first map with the venerable ggplot2.
```{r}
all_data_2018 %>% ggplot(aes(fill=disparity)) + geom_sf()
```

Once again, so easy. Amazing!  Now let's kick it up to the next level. The maps so far have been islands adrift from any geographic context.  They are also static.  We might wish to zoom in for more detail.  The first thing anybody does with Google Earth is zoom in on their own house, after all.  The advent of the `leaflet` package brings "base maps" and interactivity our maps.  This is accomplished through HTML "widgets" that bring non-R code into your workstream at the cost of potential complexity, as we shall see.  Fortunately, the fantastic `mapview` package wraps `leaflet` to make it as easy as the foregoing approaches.
```{r}
mapview(all_data_2018,
        zcol=c("disparity"),
        map.types="OpenStreetMap.BlackAndWhite",
        layer.name =  "Disparity")
```

Click, drag and scroll to zoom.  It is so easy to make great looking maps now!

# The Problem with a Univariate Approach

As wonderful as these maps are, there are problems with our analytic approach, though.  We have shoehorned two variables, income and seats into one, hard to interpret, feature.  I label it "Disparity."  Higher values mean more discrepency between the income quantile and the seats quantile, which we call "good," but there is no real intuition behind the number.  Second, much of the value of the map is lost. We don't know which zip codes are high or low income, or have plenty or few seats - just that their is a big or small difference between the values.  To really tell the story we need to show two variables simultaneously on the map.

To create a bivariate map we add a new factor that is the character combination of the income bin and the seat count bin.  Then we are done monkeying with our data set.  We can look at the zip code for Amazon's new NYC HQ to illustrate what we've got.

```{r}
ordered_combos <- c("Low, Few",
                    "Low, Average",
                    "Low, Many",
                    "Average, Few",
                    "Average, Average",
                    "Average, Many",
                    "High, Few",
                    "High, Average",
                    "High, Many")


all_data_2018 <- all_data_2018 %>% 
  mutate(IncAndSeats = factor(paste0(incomeBin,", ",seatsBin),
                        levels = ordered_combos))


all_data_2018 %>% filter(zip==11101)
```

We saw how easy it is to make a map with `mapview.`  Going forward we want more control over layers and other map features so we'll directly access the package underneath `mapview`, which is `leaflet.`

```{r message=FALSE, warning=FALSE}
# This largely follows the leaflet tutorial at
# https://rstudio.github.io/leaflet/choropleths.html

# Color scheme from http://www.joshuastevens.net/cartography/make-a-bivariate-choropleth-map/
bvColors=c("#be64ac","#8c62aa","#3b4994","#dfb0d6","#a5add3","#5698b9","#e8e8e8","#ace4e4","#5ac8c8")
pal <- colorFactor(bvColors, domain = all_data_2018$IncAndSeats)


# create the html for the data that will pop up when the mouse hovers over each zip code
popup_html <- with(
  all_data_2018,
  paste0(
    "ZIP Code :","<strong>",zip,"</strong>",
    "<br/>Income and Seats:","<strong>",IncAndSeats,"</strong>",
    "<br/>Median Income: $","<strong>",format(HouseholdIncome, big.mark = ","),"</strong>",
    "<br/>Young Children: ","<strong>",format(kidsUnder3, big.mark = ","),"</strong>",
    "<br/>Seats per 100 Kids: ","<strong>",seatsPer100Kids,"</strong>"

  )
) %>%
  lapply(htmltools::HTML)


all_data_2018 %>% 
  leaflet() %>% 
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %>% 
  addPolygons(
  fillColor = ~pal(IncAndSeats),
  weight = 1,
  opacity = 1,
  color = "black",
  dashArray = "",
  fillOpacity = 0.7,
  highlight = highlightOptions(
    weight = 5,
    color = "black",
    dashArray = "",
    fillOpacity = 0.7,
    bringToFront = TRUE),
  label = popup_html) %>% 
    addLegend(pal = pal, values = ~IncAndSeats, opacity = 0.7, title = NULL,
    position = "topleft")

```


## Custom Roll a Bi-variate Legend

So there's the map.  We can immediately see the problem with the default legend, it's the uni-dimensional nature of the legend even though we have a bi-variate color scheme.

Here is where the interesting hack comes in.  No mapping packages I am aware of have a facility for a bivariate legend.  The beautiful plots Joshua Stevens shows on his web page use a dedicated graphic composition program.  Can we cobble something up in R in `leafet`?  Yes!  The `leaflet` package allows for insertion of image layers.

To create the legend we 'simply' create a heat map of the 3x3 bins in the map and label the axes appropriately and save the image.  Then, using `addcontorl(), shove it into a corner of the map.

```{r message=FALSE}

#first create a legend plot
legendGoal = data_frame(Var1=c(rep(1:3,3)),Var2=c(1,1,1,2,2,2,3,3,3),value=1:9)
lg <- ggplot(legendGoal, aes(Var2, Var1, fill = as.factor(value))) + geom_tile()
lg <- lg + scale_fill_manual(name = "", values = bvColors)
lg <- lg + theme(legend.position = "none")
lg <- lg + theme(axis.title.x = element_text(size = rel(0.5), color = bvColors[3])) + 
  xlab(" More Income -->")
lg <- lg + theme(axis.title.y = element_text(size = rel(0.5), color = bvColors[3])) + 
  ylab("   More Seats -->")
lg <- lg + theme(axis.text = element_blank())
lg <- lg + theme(line = element_blank())
ggsave("img/bv_legend.png",lg,width = 20,height=20,units="mm")

```

##Convert the Legend Image to HTML

There is one more tricky thing we need to do.  Leaflet will create an arbitrary image layer but it expects HTML code to do it so we have to convert our image file.  Honestly, I think that's a pain that the R user shouldn't have to mess with.  The same thing applies to generating the popups like we did above.  Anyhoo, let's create a funtion to do that.
```{r}
# convert a local image file to raw base64 and embed in HTML.
# create HTML embedding a raw image

img_html <- function(img,height=100,width=100,alpha = 1){
  # style has no effect. Why?
  style <- paste0('style="opacity:',
                  alpha,
                  ';filter:alpha(opacity=',
                  alpha * 100, ')"')
  #convert an image file to base 64 to embed raw data in HTML
  img_raw <- knitr::image_uri(img)
  legend_html<- paste0('<img src="', img_raw,'"',
                       ', width=', width,',',
                       'height=', height,',',
                       style,'>') %>% 
    htmltools::HTML()
}
```
Now render the map.

```{r message=FALSE, warning=FALSE}
# This largely follows the leaflet tutorial at
# https://rstudio.github.io/leaflet/choropleths.html

pal <- colorFactor(bvColors, domain = all_data_2018$IncAndSeats)

all_data_2018 %>% 
  leaflet() %>% 
  addProviderTiles(providers$OpenStreetMap.BlackAndWhite) %>% 
  addPolygons(
  fillColor = ~pal(IncAndSeats),
  weight = 1,
  opacity = 1,
  color = "black",
  dashArray = "",
  fillOpacity = 0.7,
  highlight = highlightOptions(
    weight = 5,
    color = "black",
    dashArray = "",
    fillOpacity = 0.7,
    bringToFront = TRUE),
  label = popup_html
  ) %>% 
  addControl(img_html("img/bv_legend.png",width=150,height=150),position="topleft")

```

Bonus! Compare 2016 to 2018
```{r}
# aggregate seat count by borough


sumSeats2018 <- schools_2018 %>% 
  group_by(borough) %>% 
  summarise(schools = n(), 
            numSeats = sum(seats, na.rm = TRUE)) %>% 
  mutate(year=as.integer(2018))


load("data/alldata2016.rdata") 
all_data_2016 <-allData
rm(allData)
#2016 data doesn't have boroughs so map zips to boroughs from 2018 data
sumSeats2016<-schools_2018 %>% select(borough,zip) %>% 
  unique() %>% 
  right_join(all_data_2016,by=c("zip")) %>%
  select(borough,schools,numSeats) %>% 
  group_by(borough) %>% 
  summarise(schools = sum(schools), 
            numSeats = sum(numSeats, na.rm = TRUE)) %>% 
  mutate(year=as.integer(2016))

sumSeats <- bind_rows(sumSeats2016,sumSeats2018) %>% mutate(year=as.factor(year))
ggplot(sumSeats,aes(borough,schools,fill=year)) + 
  geom_col(position = "dodge") + 
  scale_y_continuous(labels=scales::comma) + 
  labs(title = "Expansion of NYC Free Pre-K",
       caption = "source: nyc.gov",
       x = "Borough",
       y = "Number of Schools")
```

```{r}
ggplot(sumSeats,aes(borough,numSeats,fill=year)) + 
  geom_col(position = "dodge") + 
  scale_y_continuous(labels=scales::comma) +
  labs(title = "Expansion of NYC Free Pre-K",
       caption = "source: nyc.gov",
       x = "Borough",
       y = "Number of Seats")

```

```{r}
load("data/alldata2016.rdata") 
all_data_2016 <-allData
rm(allData)

seats_delta<- all_data_2018 %>% 
  select(zip,geometry,numSeats) %>% 
  rename(numSeats_2018=numSeats) %>% 
  left_join(all_data_2016) %>% 
  na.omit() %>% 
  rename(numSeats_2016=numSeats) %>% 
  mutate(zip=zip,
         seatsPctChange16to18=round((numSeats_2018/numSeats_2016-1)*100,0)) %>% 
  select(zip,numSeats_2016,numSeats_2018,seatsPctChange16to18) %>% 
  {.}

# chop down huge percentages
seats_delta <- seats_delta %>% 
  mutate(seatsPctChange16to18 = ifelse(seatsPctChange16to18>200,200,seatsPctChange16to18))

#add spatial geometry to 2016 data
all_data_2016 <- all_data_2018 %>% 
  select(zip,geometry) %>% 
  left_join(all_data_2016) %>% 
  na.omit() %>% 
  mutate(IncAndSeats = factor(paste0(incomeBin,", ",seatsBin),
                        levels = ordered_combos))


popup_html_2016<-popupTable(all_data_2016,
           zcol=c("IncAndSeats","HouseholdIncome","kidsUnder3", "seatsPer100Kids","zip"),
           row.numbers = FALSE) %>% 
  # this does comma formatting of the HTML string for income.  Thanks, Stack Overflow!
  str_replace("(?<=\\d)(?=(\\d\\d\\d)+(?!\\d))",",") %>% 
  #get rid of superfluous features
  str_replace("<tr><td><\\/td><td><b>geometry.+\\/tr>","") %>% 
  str_replace("(<td><b>Feature ID.*?\\/tr>)(?=<tr class)","") %>% 
  # make feature labels nicer looking
  str_replace("IncAndSeats","Income and Seats") %>%
  str_replace("HouseholdIncome","Median Income") %>%
  str_replace("kidsUnder3","Young Children") %>%
  str_replace("seatsPer100Kids","Seats per 100 Kids") %>%
  str_replace("zip","ZIP Code") %>%
  {.}

mv_2016 <- mapview(all_data_2016,zcol=c("IncAndSeats"),
        popup = popup_html_2016,
        col.regions = bvColors,
        map.types="OpenStreetMap.BlackAndWhite",
        layer.name = "Income and Seats 2016",
        legend = FALSE) %>% 
    #addLogo("img/bv_legend.png",src = "local",
    #        position = "topleft",width=100,height=100,offset.x = 70) %>%
    {.}


  
  

mv_delta<- mapview(seats_delta,zcol=c("seatsPctChange16to18"),
                   popup = popup_html,
                   map.types="OpenStreetMap.BlackAndWhite",
                   layer.name = "Change in Seats\n 2016-2016") %>% 
  {.}


#mv_all %>%   leaflet::addLayersControl(overlayGroups=c("Income and Seats 2016",
#                           "Income and Seats 2018",
#                           "Change in Seats 2016-2016"))


```

Try all this in leaflet.

First create a handy helper function.  This will turn any image into inline html code that is easy to add to any of the leaflet controls.  We will use it to make our bivariate legend.  This is a very useful and reusable function.
```{r}
# create HTML embedding a raw image
img_html <- function(img,height=100,width=100,alpha = 1){
  # style has no effect. Why?
  style <- paste0('style="opacity:',
                  alpha,
                  ';filter:alpha(opacity=',
                  alpha * 100, ')"')
  #convert an image file to base 64 to embed raw data in HTML
  img_raw <- knitr::image_uri(img)
  legend_html<- paste0('<img src="',
                       img_raw,'"',
                       ', width=', width,',',
                       'height=', height,',',
                       style,'>') %>% 
    htmltools::HTML()
  return(legend_html)
}
```


